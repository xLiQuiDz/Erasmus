{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fbe8a1",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541271f",
   "metadata": {},
   "source": [
    "## Common data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f9e8e",
   "metadata": {},
   "source": [
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called `ride_sharing`. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The `user_type` column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "1. for free riders.\n",
    "2. for pay per ride.\n",
    "3. for monthly subscribers.\n",
    "In this instance, you will print the information of `ride_sharing` using `.info()` and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cf8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25762 entries, 0 to 25759\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   duration         25762 non-null  object\n",
      " 1   station_A_id     25762 non-null  int64 \n",
      " 2   station_A_name   25762 non-null  object\n",
      " 3   station_B_id     25762 non-null  int64 \n",
      " 4   station_B_name   25762 non-null  object\n",
      " 5   bike_id          25762 non-null  int64 \n",
      " 6   user_type        25762 non-null  int64 \n",
      " 7   user_birth_year  25762 non-null  int64 \n",
      " 8   user_gender      25762 non-null  object\n",
      " 9   user_type_cat    25762 non-null  int64 \n",
      " 10  tire_sizes       25762 non-null  int64 \n",
      " 11  duration_trim    25762 non-null  int64 \n",
      " 12  duration_time    25762 non-null  int64 \n",
      " 13  ride_id          25762 non-null  object\n",
      "dtypes: int64(9), object(5)\n",
      "memory usage: 2.9+ MB\n",
      "None\n",
      "count    25762.000000\n",
      "mean         2.008307\n",
      "std          0.704569\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n",
      "count     25762\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# import the ride sharing data\n",
    "ride_sharing = pd.read_csv(\"https://raw.githubusercontent.com/xLiQuiDz/Erasmus/main/Data%20Science%20-%20Lecture%202/Datasets/ride_sharing.csv\", index_col = 0)\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867334a6",
   "metadata": {},
   "source": [
    "In the previous exercise, you were able to identify that `category` is the correct data type for `user_type` and convert it in order to extract relevant statistical summaries that shed light on the distribution of `user_type`.\n",
    "\n",
    "Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column `duration` to the type `int`. Before that however, you will need to make sure to strip `\"minutes\"` from the column in order to make sure pandas reads it as numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5da1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration  duration_trim  duration_time\n",
      "0      12 minutes             12             12\n",
      "1      24 minutes             24             24\n",
      "2       8 minutes              8              8\n",
      "3       4 minutes              4              4\n",
      "4      11 minutes             11             11\n",
      "...           ...            ...            ...\n",
      "25755  11 minutes             11             11\n",
      "25756  10 minutes             10             10\n",
      "25757  14 minutes             14             14\n",
      "25758  14 minutes             14             14\n",
      "25759  29 minutes             29             29\n",
      "\n",
      "[25762 rows x 3 columns]\n",
      "11.388906140827576\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\").astype(\"int\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(\"int\")\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing[\"duration_time\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d63f3",
   "metadata": {},
   "source": [
    "In this lesson, you're going to build on top of the work you've been doing with the `ride_sharing` DataFrame. You'll be working with the `tire_sizes` column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "In this exercise, you will make sure the `tire_sizes` column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4ae391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     25762\n",
      "unique        2\n",
      "top          26\n",
      "freq      13109\n",
      "Name: tire_sizes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea209a4",
   "metadata": {},
   "source": [
    "A new update to the data pipeline feeding into `ride_sharing` has added the `ride_id` column, which represents a unique identifier for each ride. \n",
    "\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the `ride_sharing` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da712f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ride_id    duration  user_birth_year\n",
      "10329   10329   7 minutes             1970\n",
      "10329   10329   7 minutes             1970\n",
      "10566   10566  12 minutes             1992\n",
      "10566   10566  12 minutes             1992\n",
      "499       499   9 minutes             1986\n",
      "499       499  34 minutes             1997\n",
      "499       499  27 minutes             1991\n",
      "499       499  59 minutes             1985\n"
     ]
    }
   ],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(\"ride_id\", keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca668508",
   "metadata": {},
   "source": [
    "In the last exercise, you were able to verify that the new update feeding into `ride_sharing` contains a bug generating both complete and incomplete duplicated rows for some values of the `ride_id` column, with occasional discrepant values for the `user_birth_year` and `duration` columns.\n",
    "\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average `duration`, and the minimum `user_birth_year` for each set of incomplete duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a6cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration_trim': \"mean\"}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d672088",
   "metadata": {},
   "source": [
    "## Text and categorical data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa285",
   "metadata": {},
   "source": [
    "In this exercise and throughout this chapter, you'll be working with the `airlines` DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named `categories` was created, containing all correct possible values for the survey columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3af5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Somewhat sasified' 'Very saisfied' 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the airlines data\n",
    "airlines = pd.read_csv(\"https://raw.githubusercontent.com/xLiQuiDz/Erasmus/main/Data%20Science%20-%20Lecture%202/Datasets/airlines_final.csv\", index_col = 0)\n",
    "\n",
    "# Create a dictionary with all correct possible values\n",
    "categories = {\"cleanliness\" : [\"Clean\", \"Average\", \"Somewhat clean\", \"Somewhat dirty\", \"Dirty\"],\n",
    "             \"safety\": [\"Neutral\", \"Very safe\", \"Somewhat safe\", \"Very unsafe\", \"Somewhat unsafe\"],\n",
    "             \"satisfaction\" : [\"Very satisfied\", \"Neutral\", \"Somewhat satisfied\", \"Somewhat unsatisfied\", \"Very unsatisfied\"]}\n",
    "\n",
    "# Convert categories to a dataframe\n",
    "categories = pd.DataFrame(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493204b",
   "metadata": {},
   "source": [
    "Notice that in the `satisfaction` colomn there is one wrong category, \"**Somewhat sasified**\". We should tackle this inconsistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3334e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id        day         airline   destination    dest_region  dest_size  \\\n",
      "3     1157    Tuesday       SOUTHWEST   LOS ANGELES        West US        Hub   \n",
      "4     2992  Wednesday        AMERICAN         MIAMI        East US        Hub   \n",
      "6     2578   Saturday         JETBLUE    LONG BEACH        West US      Small   \n",
      "9      919     Friday      AIR CANADA       TORONTO  Canada/Mexico        Hub   \n",
      "11    1129    Tuesday       SOUTHWEST     SAN DIEGO        West US     Medium   \n",
      "...    ...        ...             ...           ...            ...        ...   \n",
      "2800  1942    Tuesday          UNITED        BOSTON        EAST US      Large   \n",
      "2801  2130   Thursday  CATHAY PACIFIC     HONG KONG           Asia        Hub   \n",
      "2803  2888  Wednesday          UNITED        AUSTIN     Midwest US     Medium   \n",
      "2804  1475    Tuesday          ALASKA  NEW YORK-JFK        East US        Hub   \n",
      "2808  2162   Saturday   CHINA EASTERN       QINGDAO           Asia      Large   \n",
      "\n",
      "     boarding_area   dept_time  wait_min     cleanliness           safety  \\\n",
      "3      Gates 20-39  2018-12-31     190.0           Clean        Very safe   \n",
      "4      Gates 50-59  2018-12-31     559.0  Somewhat clean        Very safe   \n",
      "6       Gates 1-12  2018-12-31      63.0           Clean        Very safe   \n",
      "9     Gates 91-102  2018-12-31      70.0  Somewhat clean    Somewhat safe   \n",
      "11     Gates 20-39  2018-12-31     540.0           Clean        Very safe   \n",
      "...            ...         ...       ...             ...              ...   \n",
      "2800   Gates 70-90  2018-12-31     145.0  Somewhat clean    Somewhat safe   \n",
      "2801    Gates 1-12  2018-12-31     380.0  Somewhat clean    Somewhat safe   \n",
      "2803   Gates 70-90  2018-12-31      60.0  Somewhat clean  Somewhat unsafe   \n",
      "2804   Gates 50-59  2018-12-31     280.0  Somewhat clean          Neutral   \n",
      "2808    Gates 1-12  2018-12-31     220.0           Clean        Very safe   \n",
      "\n",
      "            satisfaction  \n",
      "3     Somewhat satsified  \n",
      "4     Somewhat satsified  \n",
      "6     Somewhat satsified  \n",
      "9     Somewhat satsified  \n",
      "11    Somewhat satsified  \n",
      "...                  ...  \n",
      "2800  Somewhat satsified  \n",
      "2801  Somewhat satsified  \n",
      "2803  Somewhat satsified  \n",
      "2804  Somewhat satsified  \n",
      "2808  Somewhat satsified  \n",
      "\n",
      "[1350 rows x 12 columns]\n",
      "        id        day      airline        destination    dest_region  \\\n",
      "0     1351    Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1      373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2     2820   Thursday        DELTA        LOS ANGELES        West US   \n",
      "5      634   Thursday       ALASKA             NEWARK        East US   \n",
      "8     2592   Saturday   AEROMEXICO        MEXICO CITY  Canada/Mexico   \n",
      "...    ...        ...          ...                ...            ...   \n",
      "2799  2399  Wednesday  UNITED INTL            BEIJING           Asia   \n",
      "2802   394     Friday       ALASKA        LOS ANGELES        West US   \n",
      "2805  2222   Thursday    SOUTHWEST            PHOENIX        West US   \n",
      "2806  2684     Friday       UNITED            ORLANDO        East US   \n",
      "2807  2549    Tuesday      JETBLUE         LONG BEACH        West US   \n",
      "\n",
      "     dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0          Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1        Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2          Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "5          Hub   Gates 50-59  2018-12-31     140.0  Somewhat clean   \n",
      "8          Hub    Gates 1-12  2018-12-31     215.0  Somewhat clean   \n",
      "...        ...           ...         ...       ...             ...   \n",
      "2799       Hub  Gates 91-102  2018-12-31     195.0           Clean   \n",
      "2802       Hub   Gates 50-59  2018-12-31     115.0           Clean   \n",
      "2805       Hub   Gates 20-39  2018-12-31     165.0           Clean   \n",
      "2806       Hub   Gates 70-90  2018-12-31      92.0           Clean   \n",
      "2807     Small    Gates 1-12  2018-12-31      95.0           Clean   \n",
      "\n",
      "             safety    satisfaction  \n",
      "0           Neutral  Very satisfied  \n",
      "1         Very safe  Very satisfied  \n",
      "2     Somewhat safe         Neutral  \n",
      "5         Very safe  Very satisfied  \n",
      "8         Very safe         Neutral  \n",
      "...             ...             ...  \n",
      "2799        Neutral  Very satisfied  \n",
      "2802      Very safe  Very satisfied  \n",
      "2805      Very safe  Very satisfied  \n",
      "2806      Very safe  Very satisfied  \n",
      "2807  Somewhat safe  Very satisfied  \n",
      "\n",
      "[1127 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the satisfaction category in airlines not in categories\n",
    "cat_clean = set(airlines[\"satisfaction\"]).difference(categories[\"satisfaction\"])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['satisfaction'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4390ce",
   "metadata": {},
   "source": [
    "In this exercise, you will examine two categorical columns from this DataFrame, `dest_region` and `dest_size` respectively, assess how to address them and make sure that they are cleaned and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592c4c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1a49b",
   "metadata": {},
   "source": [
    "To better understand survey respondents from `airlines`, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The `airlines` DataFrame contains the `day` and `wait_min` columns, which are categorical and numerical respectively. The `day` column contains the exact day a flight took place, and `wait_min` contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "- `wait_type`: `'short'` for 0-60 min, `'medium'` for 60-180 and `long` for 180+\n",
    "- `day_week`: `'weekday'` if day is in the weekday, `'weekend'` if day is in the weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e831be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                               labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855bd70",
   "metadata": {},
   "source": [
    "## Advanced data problems\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the `banking` DataFrame. The dataset contains data on the amount of money stored in accounts (`acct_amount`), their currency (`acct_cur`), amount invested (`inv_amount`), account opening date (`account_opened`), and last transaction date (`last_transaction`) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1770c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking = pd.read_csv(\"https://raw.githubusercontent.com/xLiQuiDz/Erasmus/main/Data%20Science%20-%20Lecture%202/Datasets/banking_dirty.csv\", index_col = 0)\n",
    "\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67b466",
   "metadata": {},
   "source": [
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The `account_opened` column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a `datetime` object, while making sure that the format is inferred and potentially incorrect formats are set to missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5172a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    02-09-18\n",
      "1    28-02-19\n",
      "2    25-04-18\n",
      "3    07-11-17\n",
      "4    14-05-18\n",
      "Name: account_opened, dtype: object\n",
      "0     2018\n",
      "1     2019\n",
      "2     2018\n",
      "3     2017\n",
      "4     2018\n",
      "      ... \n",
      "95    2018\n",
      "96    2017\n",
      "97    2017\n",
      "98    2017\n",
      "99    2017\n",
      "Name: acct_year, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking[\"account_opened\"].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07928df5",
   "metadata": {},
   "source": [
    "New data has been merged into the `banking` DataFrame that contains details on how investments in the `inv_amount` column are allocated across four different funds A, B, C and D. \n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the `age` and `birth_date` columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of `inv_amount` and `age` against the amount invested in different funds and customers' birthdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1074acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent investments:  10\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[[\"fund_A\", \"fund_B\", \"fund_C\", \"fund_D\"]].sum(axis = 1) == banking[\"inv_amount\"]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dc0e0",
   "metadata": {},
   "source": [
    "Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data. You just received a new version of the `banking` DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing `inv_amount` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253ee168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_id             0\n",
      "birth_date          0\n",
      "Age                 0\n",
      "acct_amount         0\n",
      "inv_amount          2\n",
      "fund_A              0\n",
      "fund_B              0\n",
      "fund_C              0\n",
      "fund_D              0\n",
      "account_opened      0\n",
      "last_transaction    0\n",
      "acct_cur            0\n",
      "acct_year           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
